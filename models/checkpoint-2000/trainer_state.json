{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 285.7142857142857,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.857142857142857,
      "grad_norm": 2.281505823135376,
      "learning_rate": 0.001980952380952381,
      "loss": 2.8289,
      "step": 20
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 3.379202127456665,
      "learning_rate": 0.001961904761904762,
      "loss": 2.1402,
      "step": 40
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 10.04445743560791,
      "learning_rate": 0.0019428571428571429,
      "loss": 1.6956,
      "step": 60
    },
    {
      "epoch": 11.428571428571429,
      "grad_norm": 8.572478294372559,
      "learning_rate": 0.0019238095238095238,
      "loss": 1.3468,
      "step": 80
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 9.565925598144531,
      "learning_rate": 0.0019047619047619048,
      "loss": 0.9633,
      "step": 100
    },
    {
      "epoch": 17.142857142857142,
      "grad_norm": 6.118987083435059,
      "learning_rate": 0.0018857142857142857,
      "loss": 0.6729,
      "step": 120
    },
    {
      "epoch": 20.0,
      "grad_norm": 16.664335250854492,
      "learning_rate": 0.0018666666666666666,
      "loss": 0.4792,
      "step": 140
    },
    {
      "epoch": 22.857142857142858,
      "grad_norm": 9.520157814025879,
      "learning_rate": 0.0018476190476190478,
      "loss": 0.3404,
      "step": 160
    },
    {
      "epoch": 25.714285714285715,
      "grad_norm": 10.662841796875,
      "learning_rate": 0.0018285714285714285,
      "loss": 0.3753,
      "step": 180
    },
    {
      "epoch": 28.571428571428573,
      "grad_norm": 2.7262821197509766,
      "learning_rate": 0.0018095238095238095,
      "loss": 0.431,
      "step": 200
    },
    {
      "epoch": 31.428571428571427,
      "grad_norm": 16.274415969848633,
      "learning_rate": 0.0017904761904761906,
      "loss": 0.3221,
      "step": 220
    },
    {
      "epoch": 34.285714285714285,
      "grad_norm": 2.5779542922973633,
      "learning_rate": 0.0017714285714285714,
      "loss": 0.2492,
      "step": 240
    },
    {
      "epoch": 37.142857142857146,
      "grad_norm": 3.315668821334839,
      "learning_rate": 0.0017523809523809523,
      "loss": 0.1112,
      "step": 260
    },
    {
      "epoch": 40.0,
      "grad_norm": 15.17422103881836,
      "learning_rate": 0.0017333333333333335,
      "loss": 0.1369,
      "step": 280
    },
    {
      "epoch": 42.857142857142854,
      "grad_norm": 7.097811222076416,
      "learning_rate": 0.0017142857142857142,
      "loss": 0.0826,
      "step": 300
    },
    {
      "epoch": 45.714285714285715,
      "grad_norm": 1.6196726560592651,
      "learning_rate": 0.0016952380952380951,
      "loss": 0.091,
      "step": 320
    },
    {
      "epoch": 48.57142857142857,
      "grad_norm": 4.536976337432861,
      "learning_rate": 0.0016761904761904763,
      "loss": 0.0723,
      "step": 340
    },
    {
      "epoch": 51.42857142857143,
      "grad_norm": 6.733476161956787,
      "learning_rate": 0.0016571428571428572,
      "loss": 0.068,
      "step": 360
    },
    {
      "epoch": 54.285714285714285,
      "grad_norm": 3.214463233947754,
      "learning_rate": 0.0016380952380952382,
      "loss": 0.0634,
      "step": 380
    },
    {
      "epoch": 57.142857142857146,
      "grad_norm": 1.5080442428588867,
      "learning_rate": 0.0016190476190476191,
      "loss": 0.0613,
      "step": 400
    },
    {
      "epoch": 60.0,
      "grad_norm": 17.544689178466797,
      "learning_rate": 0.0016,
      "loss": 0.3178,
      "step": 420
    },
    {
      "epoch": 62.857142857142854,
      "grad_norm": 0.8450155854225159,
      "learning_rate": 0.001580952380952381,
      "loss": 0.0648,
      "step": 440
    },
    {
      "epoch": 65.71428571428571,
      "grad_norm": 0.8156876564025879,
      "learning_rate": 0.001561904761904762,
      "loss": 0.0359,
      "step": 460
    },
    {
      "epoch": 68.57142857142857,
      "grad_norm": 0.5041487216949463,
      "learning_rate": 0.001542857142857143,
      "loss": 0.0192,
      "step": 480
    },
    {
      "epoch": 71.42857142857143,
      "grad_norm": 0.19729959964752197,
      "learning_rate": 0.0015238095238095239,
      "loss": 0.0136,
      "step": 500
    },
    {
      "epoch": 74.28571428571429,
      "grad_norm": 6.8092193603515625,
      "learning_rate": 0.0015047619047619048,
      "loss": 0.0254,
      "step": 520
    },
    {
      "epoch": 77.14285714285714,
      "grad_norm": 0.25161653757095337,
      "learning_rate": 0.0014857142857142857,
      "loss": 0.0169,
      "step": 540
    },
    {
      "epoch": 80.0,
      "grad_norm": 6.513935565948486,
      "learning_rate": 0.0014666666666666667,
      "loss": 0.0209,
      "step": 560
    },
    {
      "epoch": 82.85714285714286,
      "grad_norm": 5.638925552368164,
      "learning_rate": 0.0014476190476190476,
      "loss": 0.0239,
      "step": 580
    },
    {
      "epoch": 85.71428571428571,
      "grad_norm": 3.658987283706665,
      "learning_rate": 0.0014285714285714286,
      "loss": 0.0185,
      "step": 600
    },
    {
      "epoch": 88.57142857142857,
      "grad_norm": 0.7330140471458435,
      "learning_rate": 0.0014095238095238097,
      "loss": 0.0162,
      "step": 620
    },
    {
      "epoch": 91.42857142857143,
      "grad_norm": 2.90089750289917,
      "learning_rate": 0.0013904761904761905,
      "loss": 0.018,
      "step": 640
    },
    {
      "epoch": 94.28571428571429,
      "grad_norm": 0.10749298334121704,
      "learning_rate": 0.0013714285714285714,
      "loss": 0.031,
      "step": 660
    },
    {
      "epoch": 97.14285714285714,
      "grad_norm": 0.3664778769016266,
      "learning_rate": 0.0013523809523809526,
      "loss": 0.0196,
      "step": 680
    },
    {
      "epoch": 100.0,
      "grad_norm": 1.9009069204330444,
      "learning_rate": 0.0013333333333333333,
      "loss": 0.0103,
      "step": 700
    },
    {
      "epoch": 102.85714285714286,
      "grad_norm": 0.24676522612571716,
      "learning_rate": 0.0013142857142857142,
      "loss": 0.0153,
      "step": 720
    },
    {
      "epoch": 105.71428571428571,
      "grad_norm": 2.823941707611084,
      "learning_rate": 0.0012952380952380954,
      "loss": 0.0036,
      "step": 740
    },
    {
      "epoch": 108.57142857142857,
      "grad_norm": 0.04044925794005394,
      "learning_rate": 0.0012761904761904761,
      "loss": 0.0019,
      "step": 760
    },
    {
      "epoch": 111.42857142857143,
      "grad_norm": 0.04449084401130676,
      "learning_rate": 0.001257142857142857,
      "loss": 0.0017,
      "step": 780
    },
    {
      "epoch": 114.28571428571429,
      "grad_norm": 0.08139801770448685,
      "learning_rate": 0.0012380952380952382,
      "loss": 0.0018,
      "step": 800
    },
    {
      "epoch": 117.14285714285714,
      "grad_norm": 0.0370609350502491,
      "learning_rate": 0.0012190476190476192,
      "loss": 0.0017,
      "step": 820
    },
    {
      "epoch": 120.0,
      "grad_norm": 0.021155573427677155,
      "learning_rate": 0.0012,
      "loss": 0.0082,
      "step": 840
    },
    {
      "epoch": 122.85714285714286,
      "grad_norm": 0.03113732673227787,
      "learning_rate": 0.001180952380952381,
      "loss": 0.0012,
      "step": 860
    },
    {
      "epoch": 125.71428571428571,
      "grad_norm": 0.028134433552622795,
      "learning_rate": 0.001161904761904762,
      "loss": 0.0009,
      "step": 880
    },
    {
      "epoch": 128.57142857142858,
      "grad_norm": 0.013116761110723019,
      "learning_rate": 0.0011428571428571427,
      "loss": 0.0006,
      "step": 900
    },
    {
      "epoch": 131.42857142857142,
      "grad_norm": 0.012383531779050827,
      "learning_rate": 0.0011238095238095239,
      "loss": 0.0006,
      "step": 920
    },
    {
      "epoch": 134.28571428571428,
      "grad_norm": 0.01043042354285717,
      "learning_rate": 0.0011047619047619048,
      "loss": 0.0005,
      "step": 940
    },
    {
      "epoch": 137.14285714285714,
      "grad_norm": 0.010391077026724815,
      "learning_rate": 0.0010857142857142856,
      "loss": 0.0005,
      "step": 960
    },
    {
      "epoch": 140.0,
      "grad_norm": 0.023343121632933617,
      "learning_rate": 0.0010666666666666667,
      "loss": 0.0004,
      "step": 980
    },
    {
      "epoch": 142.85714285714286,
      "grad_norm": 0.00966445542871952,
      "learning_rate": 0.0010476190476190477,
      "loss": 0.0004,
      "step": 1000
    },
    {
      "epoch": 145.71428571428572,
      "grad_norm": 0.008721987716853619,
      "learning_rate": 0.0010285714285714284,
      "loss": 0.0004,
      "step": 1020
    },
    {
      "epoch": 148.57142857142858,
      "grad_norm": 0.007956636138260365,
      "learning_rate": 0.0010095238095238095,
      "loss": 0.0004,
      "step": 1040
    },
    {
      "epoch": 151.42857142857142,
      "grad_norm": 0.007245440501719713,
      "learning_rate": 0.0009904761904761905,
      "loss": 0.0004,
      "step": 1060
    },
    {
      "epoch": 154.28571428571428,
      "grad_norm": 0.006167840678244829,
      "learning_rate": 0.0009714285714285714,
      "loss": 0.0004,
      "step": 1080
    },
    {
      "epoch": 157.14285714285714,
      "grad_norm": 0.008999057114124298,
      "learning_rate": 0.0009523809523809524,
      "loss": 0.0003,
      "step": 1100
    },
    {
      "epoch": 160.0,
      "grad_norm": 0.00848179217427969,
      "learning_rate": 0.0009333333333333333,
      "loss": 0.0003,
      "step": 1120
    },
    {
      "epoch": 162.85714285714286,
      "grad_norm": 0.007977410219609737,
      "learning_rate": 0.0009142857142857143,
      "loss": 0.0003,
      "step": 1140
    },
    {
      "epoch": 165.71428571428572,
      "grad_norm": 0.009898105636239052,
      "learning_rate": 0.0008952380952380953,
      "loss": 0.0003,
      "step": 1160
    },
    {
      "epoch": 168.57142857142858,
      "grad_norm": 0.007578632328659296,
      "learning_rate": 0.0008761904761904762,
      "loss": 0.0003,
      "step": 1180
    },
    {
      "epoch": 171.42857142857142,
      "grad_norm": 0.006661061197519302,
      "learning_rate": 0.0008571428571428571,
      "loss": 0.0003,
      "step": 1200
    },
    {
      "epoch": 174.28571428571428,
      "grad_norm": 0.009049313142895699,
      "learning_rate": 0.0008380952380952382,
      "loss": 0.0003,
      "step": 1220
    },
    {
      "epoch": 177.14285714285714,
      "grad_norm": 0.008532408624887466,
      "learning_rate": 0.0008190476190476191,
      "loss": 0.0003,
      "step": 1240
    },
    {
      "epoch": 180.0,
      "grad_norm": 0.009003858081996441,
      "learning_rate": 0.0008,
      "loss": 0.0003,
      "step": 1260
    },
    {
      "epoch": 182.85714285714286,
      "grad_norm": 0.007358016911894083,
      "learning_rate": 0.000780952380952381,
      "loss": 0.0003,
      "step": 1280
    },
    {
      "epoch": 185.71428571428572,
      "grad_norm": 0.007656077854335308,
      "learning_rate": 0.0007619047619047619,
      "loss": 0.0003,
      "step": 1300
    },
    {
      "epoch": 188.57142857142858,
      "grad_norm": 0.004367633257061243,
      "learning_rate": 0.0007428571428571429,
      "loss": 0.0003,
      "step": 1320
    },
    {
      "epoch": 191.42857142857142,
      "grad_norm": 0.0053154644556343555,
      "learning_rate": 0.0007238095238095238,
      "loss": 0.0003,
      "step": 1340
    },
    {
      "epoch": 194.28571428571428,
      "grad_norm": 0.00512306671589613,
      "learning_rate": 0.0007047619047619049,
      "loss": 0.0002,
      "step": 1360
    },
    {
      "epoch": 197.14285714285714,
      "grad_norm": 0.005575242917984724,
      "learning_rate": 0.0006857142857142857,
      "loss": 0.0002,
      "step": 1380
    },
    {
      "epoch": 200.0,
      "grad_norm": 0.006554069463163614,
      "learning_rate": 0.0006666666666666666,
      "loss": 0.0002,
      "step": 1400
    },
    {
      "epoch": 202.85714285714286,
      "grad_norm": 0.00485611567273736,
      "learning_rate": 0.0006476190476190477,
      "loss": 0.0002,
      "step": 1420
    },
    {
      "epoch": 205.71428571428572,
      "grad_norm": 0.004348213318735361,
      "learning_rate": 0.0006285714285714285,
      "loss": 0.0002,
      "step": 1440
    },
    {
      "epoch": 208.57142857142858,
      "grad_norm": 0.0053118010982871056,
      "learning_rate": 0.0006095238095238096,
      "loss": 0.0002,
      "step": 1460
    },
    {
      "epoch": 211.42857142857142,
      "grad_norm": 0.004748984705656767,
      "learning_rate": 0.0005904761904761905,
      "loss": 0.0002,
      "step": 1480
    },
    {
      "epoch": 214.28571428571428,
      "grad_norm": 0.00410920474678278,
      "learning_rate": 0.0005714285714285714,
      "loss": 0.0002,
      "step": 1500
    },
    {
      "epoch": 217.14285714285714,
      "grad_norm": 0.004771089181303978,
      "learning_rate": 0.0005523809523809524,
      "loss": 0.0002,
      "step": 1520
    },
    {
      "epoch": 220.0,
      "grad_norm": 0.007591488305479288,
      "learning_rate": 0.0005333333333333334,
      "loss": 0.0002,
      "step": 1540
    },
    {
      "epoch": 222.85714285714286,
      "grad_norm": 0.005102214403450489,
      "learning_rate": 0.0005142857142857142,
      "loss": 0.0002,
      "step": 1560
    },
    {
      "epoch": 225.71428571428572,
      "grad_norm": 0.004472974222153425,
      "learning_rate": 0.0004952380952380952,
      "loss": 0.0002,
      "step": 1580
    },
    {
      "epoch": 228.57142857142858,
      "grad_norm": 0.003780805505812168,
      "learning_rate": 0.0004761904761904762,
      "loss": 0.0002,
      "step": 1600
    },
    {
      "epoch": 231.42857142857142,
      "grad_norm": 0.006095291115343571,
      "learning_rate": 0.00045714285714285713,
      "loss": 0.0002,
      "step": 1620
    },
    {
      "epoch": 234.28571428571428,
      "grad_norm": 0.004750058986246586,
      "learning_rate": 0.0004380952380952381,
      "loss": 0.0002,
      "step": 1640
    },
    {
      "epoch": 237.14285714285714,
      "grad_norm": 0.005132889840751886,
      "learning_rate": 0.0004190476190476191,
      "loss": 0.0002,
      "step": 1660
    },
    {
      "epoch": 240.0,
      "grad_norm": 0.004969603382050991,
      "learning_rate": 0.0004,
      "loss": 0.0002,
      "step": 1680
    },
    {
      "epoch": 242.85714285714286,
      "grad_norm": 0.004597146529704332,
      "learning_rate": 0.00038095238095238096,
      "loss": 0.0002,
      "step": 1700
    },
    {
      "epoch": 245.71428571428572,
      "grad_norm": 0.00543043902143836,
      "learning_rate": 0.0003619047619047619,
      "loss": 0.0002,
      "step": 1720
    },
    {
      "epoch": 248.57142857142858,
      "grad_norm": 0.004111603833734989,
      "learning_rate": 0.00034285714285714285,
      "loss": 0.0002,
      "step": 1740
    },
    {
      "epoch": 251.42857142857142,
      "grad_norm": 0.004540775436908007,
      "learning_rate": 0.00032380952380952385,
      "loss": 0.0002,
      "step": 1760
    },
    {
      "epoch": 254.28571428571428,
      "grad_norm": 0.004812008701264858,
      "learning_rate": 0.0003047619047619048,
      "loss": 0.0002,
      "step": 1780
    },
    {
      "epoch": 257.14285714285717,
      "grad_norm": 0.003490994917228818,
      "learning_rate": 0.0002857142857142857,
      "loss": 0.0002,
      "step": 1800
    },
    {
      "epoch": 260.0,
      "grad_norm": 0.003605545498430729,
      "learning_rate": 0.0002666666666666667,
      "loss": 0.0002,
      "step": 1820
    },
    {
      "epoch": 262.85714285714283,
      "grad_norm": 0.005670445971190929,
      "learning_rate": 0.0002476190476190476,
      "loss": 0.0002,
      "step": 1840
    },
    {
      "epoch": 265.7142857142857,
      "grad_norm": 0.003823456121608615,
      "learning_rate": 0.00022857142857142857,
      "loss": 0.0002,
      "step": 1860
    },
    {
      "epoch": 268.57142857142856,
      "grad_norm": 0.0038114874623715878,
      "learning_rate": 0.00020952380952380954,
      "loss": 0.0002,
      "step": 1880
    },
    {
      "epoch": 271.42857142857144,
      "grad_norm": 0.004447664134204388,
      "learning_rate": 0.00019047619047619048,
      "loss": 0.0002,
      "step": 1900
    },
    {
      "epoch": 274.2857142857143,
      "grad_norm": 0.0033532208763062954,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.0002,
      "step": 1920
    },
    {
      "epoch": 277.14285714285717,
      "grad_norm": 0.00549335777759552,
      "learning_rate": 0.0001523809523809524,
      "loss": 0.0002,
      "step": 1940
    },
    {
      "epoch": 280.0,
      "grad_norm": 0.005508071277290583,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.0002,
      "step": 1960
    },
    {
      "epoch": 282.85714285714283,
      "grad_norm": 0.005296069663017988,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.0002,
      "step": 1980
    },
    {
      "epoch": 285.7142857142857,
      "grad_norm": 0.0046484386548399925,
      "learning_rate": 9.523809523809524e-05,
      "loss": 0.0002,
      "step": 2000
    }
  ],
  "logging_steps": 20,
  "max_steps": 2100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 300,
  "save_steps": 500,
  "total_flos": 0.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
